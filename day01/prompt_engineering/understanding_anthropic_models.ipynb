{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic Model List - Jupyter Notebook\n",
    "\n",
    "This notebook demonstrates how to retrieve and display a list of available models from the Anthropic AI platform.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this code, make sure you have:\n",
    "\n",
    "1. Installed the Anthropic Python client:\n",
    "   ```\n",
    "   !pip install anthropic\n",
    "   ```\n",
    "\n",
    "2. Set your Anthropic API key as an environment variable:\n",
    "   ```\n",
    "   import os\n",
    "   os.environ[\"ANTHROPIC_API_KEY\"] = \"your_api_key_here\"  # Replace with your actual API key\n",
    "   ```\n",
    "   \n",
    "   Alternatively, you can create a `.env` file and load it:\n",
    "   ```\n",
    "   from dotenv import load_dotenv\n",
    "   load_dotenv()\n",
    "   ```\n",
    "\n",
    "## Retrieving and Displaying Anthropic Models\n",
    "\n",
    "The code below will:\n",
    "1. Initialize the Anthropic client\n",
    "2. Retrieve a list of available models (limited to 20)\n",
    "3. Display each model's ID and display name in a formatted way\n",
    "\n",
    "```python\n",
    "# Import the Anthropic library\n",
    "import anthropic\n",
    "\n",
    "# Initialize the client\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# Get the models\n",
    "models = client.models.list(limit=20)\n",
    "\n",
    "# Print model ID and display name\n",
    "print(\"Available Anthropic Models:\")\n",
    "print(\"--------------------------\")\n",
    "for model in models.data:\n",
    "    print(f\"{model.id:<35} | {model.display_name}\")\n",
    "```\n",
    "\n",
    "## How It Works\n",
    "\n",
    "- The script uses the official Anthropic Python client to interact with their API\n",
    "- We limit the results to 20 models, although Anthropic typically offers fewer than that\n",
    "- For each model, we display:\n",
    "  - The model ID (e.g., \"claude-3-7-sonnet-20250219\") - this is what you use in API calls\n",
    "  - The human-readable display name (e.g., \"Claude 3.7 Sonnet\")\n",
    "- The `:<35` in the f-string is a formatting instruction that left-aligns the model ID text and pads it to 35 characters, creating a clean output\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "When run, this code should produce output similar to:\n",
    "\n",
    "```\n",
    "Available Anthropic Models:\n",
    "--------------------------\n",
    "claude-3-7-sonnet-20250219         | Claude 3.7 Sonnet\n",
    "claude-3-5-sonnet-20240620         | Claude 3.5 Sonnet\n",
    "claude-3-opus-20240229             | Claude 3 Opus\n",
    "claude-3-sonnet-20240229           | Claude 3 Sonnet\n",
    "claude-2.1                         | Claude 2.1\n",
    "```\n",
    "\n",
    "## Enhanced Display (Optional)\n",
    "\n",
    "If you want a more visually appealing display, you can use the `tabulate` library:\n",
    "\n",
    "```python\n",
    "import anthropic\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Initialize the client\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# Get the models\n",
    "models = client.models.list(limit=20)\n",
    "\n",
    "# Prepare data for tabulation\n",
    "model_data = []\n",
    "for model in models.data:\n",
    "    # Format the date to be more readable\n",
    "    created_date = model.created_at.strftime(\"%Y-%m-%d\")\n",
    "    model_data.append([model.id, model.display_name, created_date])\n",
    "\n",
    "# Print as a table\n",
    "print(tabulate(model_data, headers=[\"Model ID\", \"Display Name\", \"Created Date\"], tablefmt=\"grid\"))\n",
    "```\n",
    "\n",
    "Make sure to install tabulate first with `!pip install tabulate` if you choose this option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Anthropic Models:\n",
      "--------------------------\n",
      "claude-opus-4-20250514              | Claude Opus 4\n",
      "claude-sonnet-4-20250514            | Claude Sonnet 4\n",
      "claude-3-7-sonnet-20250219          | Claude Sonnet 3.7\n",
      "claude-3-5-sonnet-20241022          | Claude Sonnet 3.5 (New)\n",
      "claude-3-5-haiku-20241022           | Claude Haiku 3.5\n",
      "claude-3-5-sonnet-20240620          | Claude Sonnet 3.5 (Old)\n",
      "claude-3-haiku-20240307             | Claude Haiku 3\n",
      "claude-3-opus-20240229              | Claude Opus 3\n",
      "claude-3-sonnet-20240229            | Claude Sonnet 3\n",
      "claude-2.1                          | Claude 2.1\n",
      "claude-2.0                          | Claude 2.0\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# Initialize the client\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# Get the models\n",
    "models = client.models.list(limit=20)\n",
    "\n",
    "# Print model ID and display name\n",
    "print(\"Available Anthropic Models:\")\n",
    "print(\"--------------------------\")\n",
    "for model in models.data:\n",
    "    print(f\"{model.id:<35} | {model.display_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API keys are successfully loaded.\n",
      "ðŸ”’ API keys are loaded but hidden for security.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Check if API keys are loaded\n",
    "if anthropic_api_key:\n",
    "    print(\"âœ… API keys are successfully loaded.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: One or more API keys are missing.\")\n",
    "\n",
    "# Optionally, display API keys (for debugging purposes only)\n",
    "display_keys = False  # Change to True if you want to see the keys\n",
    "\n",
    "if display_keys:\n",
    "    print(f\"Anthropic API Key: {anthropic_api_key}\")\n",
    "else:\n",
    "    print(\"ðŸ”’ API keys are loaded but hidden for security.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"claude-3-5-haiku-20241022\"\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "def get_completion(prompt: str):\n",
    "    message = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Hello, Claude!\"\n",
    "\n",
    "# Get Claude's response\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Model Comparison Guide\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "Anthropic offers several Claude models, each optimized for different use cases. Here's a comprehensive comparison to help you choose the right model for your needs. This is a rough guide to help you think about pricing. \n",
    "\n",
    "### Claude 4 Series (Latest)\n",
    "\n",
    "| Model | Context Window | Strengths | Best For | Pricing (per M tokens) |\n",
    "|-------|----------------|-----------|----------|------------------------|\n",
    "| **Claude Opus 4** | 200K tokens | Most capable, advanced reasoning, complex tasks | Research, analysis, coding, creative writing | Input: $15, Output: $75 |\n",
    "| **Claude Sonnet 4** | 200K tokens | Balanced performance and speed | General purpose, business applications | Input: $3, Output: $15 |\n",
    "\n",
    "### Claude 3 Series\n",
    "\n",
    "| Model | Context Window | Strengths | Best For | Pricing (per M tokens) |\n",
    "|-------|----------------|-----------|----------|------------------------|\n",
    "| **Claude 3.7 Sonnet** | 200K tokens | Enhanced reasoning, improved coding | Development, technical writing | Input: $3, Output: $15 |\n",
    "| **Claude 3.5 Sonnet (New)** | 200K tokens | Fast, versatile, good at reasoning | Most general use cases | Input: $3, Output: $15 |\n",
    "| **Claude 3.5 Haiku** | 200K tokens | Fastest, cost-effective | Simple tasks, high-volume processing | Input: $0.25, Output: $1.25 |\n",
    "| **Claude 3.5 Sonnet (Old)** | 200K tokens | Previous generation Sonnet | Legacy applications | Input: $3, Output: $15 |\n",
    "| **Claude 3 Opus** | 200K tokens | Most capable 3.x model | Complex reasoning, research | Input: $15, Output: $75 |\n",
    "| **Claude 3 Sonnet** | 200K tokens | Balanced 3.x model | General purpose | Input: $3, Output: $15 |\n",
    "| **Claude 3 Haiku** | 200K tokens | Fastest 3.x model | Simple, quick tasks | Input: $0.25, Output: $1.25 |\n",
    "\n",
    "## Model Selection Guide\n",
    "\n",
    "### ðŸŽ¯ **For Production Applications**\n",
    "- **Claude Sonnet 4**: Best balance of capability and cost\n",
    "- **Claude 3.5 Sonnet (New)**: Proven performance, widely adopted\n",
    "\n",
    "### âš¡ **For High-Volume/Speed-Critical Tasks**\n",
    "- **Claude 3.5 Haiku**: Fastest response times, most cost-effective\n",
    "- **Claude 3 Haiku**: Alternative for legacy systems\n",
    "\n",
    "### ðŸ§  **For Complex Reasoning & Analysis**\n",
    "- **Claude Opus 4**: Cutting-edge capabilities\n",
    "- **Claude 3 Opus**: Proven complex reasoning abilities\n",
    "\n",
    "### ðŸ’» **For Coding & Technical Tasks**\n",
    "- **Claude 3.7 Sonnet**: Enhanced for development workflows\n",
    "- **Claude Sonnet 4**: Advanced coding capabilities\n",
    "\n",
    "### ðŸ“ **For Creative Writing & Content**\n",
    "- **Claude Opus 4**: Most creative and nuanced\n",
    "- **Claude 3.5 Sonnet**: Good creative balance\n",
    "\n",
    "## Key Considerations\n",
    "\n",
    "### **Context Window**\n",
    "- All current models support 200K tokens (~150K words)\n",
    "- Ideal for processing long documents, codebases, or conversations\n",
    "\n",
    "### **Response Quality vs Speed**\n",
    "- **Opus models**: Highest quality, slower responses\n",
    "- **Sonnet models**: Balanced quality and speed\n",
    "- **Haiku models**: Fastest responses, good quality for simpler tasks\n",
    "\n",
    "### **Cost Optimization**\n",
    "- Start with **Claude 3.5 Haiku** for prototyping\n",
    "- Upgrade to **Sonnet** models for production\n",
    "- Use **Opus** models only when maximum capability is required\n",
    "\n",
    "### **Model Updates**\n",
    "- Claude 4 series represents the latest generation\n",
    "- Claude 3.5 models receive periodic updates\n",
    "- Always test new models before switching production systems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages format\n",
    "\n",
    "As we saw in the previous lesson, we can use `client.messages.create()` to send a message to Claude and get a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_014WE4hqyZBF7Ynw3eTG39SF', content=[TextBlock(text='The exact recipe for Coca-Cola is a closely guarded trade secret, but the general flavors and ingredients are known:\\n\\n- Carbonated water - This provides the bubbly carbonation.\\n\\n- Caffeine - Coca-Cola contains around 34mg of caffeine per 12oz serving.\\n\\n- Sugar (or high fructose corn syrup) - This provides the sweetness.\\n\\n- Caramel coloring - This gives Coca-Cola its distinctive brown color.\\n\\n- Phosphoric acid - This adds tartness and a slightly acidic flavor.\\n\\nThe specific flavors used are a blend of citrus flavors (such as lemon, lime, and orange), spices (such as cinnamon and nutmeg), and other flavorings. The original Coca-Cola formula also contained coca leaf extract, which provided a small amount of cocaine, but this has been removed since the early 1900s.\\n\\nThe exact blend of these flavors is what gives Coca-Cola its unique taste, and the full recipe has been kept secret by the Coca-Cola company since its creation in 1886.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=17, output_tokens=247, service_tier='standard'))\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavors are used in Coca Cola?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this bit: \n",
    "```py\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavors are used in Dr. Pepper?\"}\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The messages parameter is a crucial part of interacting with the Claude API. It allows you to provide the conversation history and context for Claude to generate a relevant response. \n",
    "\n",
    "The messages parameter expects a list of message dictionaries, where each dictionary represents a single message in the conversation.\n",
    "Each message dictionary should have the following keys:\n",
    "\n",
    "* `role`: A string indicating the role of the message sender. It can be either \"user\" (for messages sent by the user) or \"assistant\" (for messages sent by Claude).\n",
    "* `content`: A string or list of content dictionaries representing the actual content of the message. If a string is provided, it will be treated as a single text content block. If a list of content dictionaries is provided, each dictionary should have a \"type\" (e.g., \"text\" or \"image\") and the corresponding content.  For now, we'll leave `content` as a single string.\n",
    "\n",
    "Here's an example of a messages list with a single user message:\n",
    "\n",
    "```py\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello Claude! How are you today?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "And here's an example with multiple messages representing a conversation:\n",
    "\n",
    "```py\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello Claude! How are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! I'm doing well, thank you. How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me a fun fact about ferrets?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Did you know that excited ferrets make a clucking vocalization known as 'dooking'?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Remember that messages always alternate between user and assistant messages (Source of Image: Anthropic Courses).\n",
    "\n",
    "![Alternating Messages](images/alternating_messages.png)\n",
    "\n",
    "The messages format allows us to structure our API calls to Claude in the form of a conversation, allowing for **context preservation**: The messages format allows for maintaining an entire conversation history, including both user and assistant messages. This ensures that Claude has access to the full context of the conversation when generating responses, leading to more coherent and relevant outputs.  \n",
    "\n",
    "**Note: many use-cases don't require a conversation history, and there's nothing wrong with providing a list of messages that only contains a single message!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Inspecting the message response\n",
    "Next, let's take a look at the shape of the response we get back from Claude. \n",
    "\n",
    "Let's ask Claude to do something simple and now let's inspect the contents of the `response` that we get back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(id='msg_01WapibPPHkyoxMZYvJAEwQ8', content=[TextBlock(text='Merci.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=20, output_tokens=7, service_tier='standard'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Translate Thank You to French. Respond with a single word\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back a `Message` object that contains a handful of properties.  Here's an example:\n",
    "\n",
    "```\n",
    "Message(id='msg_01Mq5gDnUmDESukTgwPV8xtG', content=[TextBlock(text='Bonjour.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=19, output_tokens=8))\n",
    "```\n",
    "\n",
    " The most important piece of information is the `content` property: this contains the actual content the model generated for us.   This is a **list** of content blocks, each of which has a type that determines its shape.\n",
    "\n",
    " In order to access the actual text content of the model's response, we need to do the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merci.\n"
     ]
    }
   ],
   "source": [
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `content`, the `Message` object contains some other pieces of information:\n",
    "\n",
    "* `id` - a unique object identifier\n",
    "* `type` - The object type, which will always be \"message\"\n",
    "* `role` - The conversational role of the generated message. This will always be \"assistant\".\n",
    "* `model` - The model that handled the request and generated the response\n",
    "* `stop_reason` - The reason the model stopped generating.  We'll learn more about this later.\n",
    "* `stop_sequence` - We'll learn more about this shortly.\n",
    "* `usage` - information on billing and rate-limit usage. Contains information on:\n",
    "    * `input_tokens` - The number of input tokens that were used.\n",
    "    * `output_tokens` - The number of output tokens that were used.\n",
    "\n",
    "It's important to know that we have access to these pieces of information, but if you only remember one thing, make it this: `content` contains the actual model-generated content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages list use cases\n",
    "\n",
    "The messages list is a powerful feature that allows you to build complex interactions with Claude. Here are some common use cases:\n",
    "\n",
    "### Putting words in Claude's mouth\n",
    "\n",
    "Another common strategy for getting very specific outputs is to \"put words in Claude's mouth\".  Instead of only providing `user` messages to Claude, we can also supply an `assistant` message that Claude will use when generating output.  \n",
    "\n",
    "When using Anthropicâ€™s API, you are not limited to just the `user` message. If you supply an `assistant` message, Claude will continue the conversation from the last `assistant` token.  Just remember that we must start with a `user` message.\n",
    "\n",
    "Suppose I want Claude to write me a haiku that starts with the first line, \"calming mountain air\".  I can provide the following conversation history: \n",
    "\n",
    "```py\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Generate a beautiful haiku\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"calming mountain air\"}\n",
    "    ]\n",
    "```\n",
    "We tell Claude that we want it to generate a Haiku AND we put the first line of the Haiku in Claude's mouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "petals fall like gentle tears\n",
      "nature's soothing dance\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Generate a beautiful haiku\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"calming mountain air\"}\n",
    "    ]\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the entire haiku, starting with the line we provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calming mountain air\n",
      "petals fall like gentle tears\n",
      "nature's soothing dance\n"
     ]
    }
   ],
   "source": [
    "print(\"calming mountain air\" + response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "One of the most useful prompting strategies is called \"few-shot prompting\" which involves providing a model with a small number of **examples**.  These examples help guide Claude's generated output.  The messages conversation history is an easy way to provide examples to Claude.\n",
    "\n",
    "For example, suppose we want to use Claude to analyze the sentiment in tweets.  We could start by simply asking Claude to \"please analyze the sentiment in this tweet: \" and see what sort of output we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment in this tweet is positive. Here's a breakdown of the analysis:\n",
      "\n",
      "1. Positive language: The tweet uses phrases like \"doing a happy dance\" and \"pickleslove\" which convey a sense of excitement and enjoyment.\n",
      "\n",
      "2. Emojis: The use of the pepper and pickle emojis add a playful and enthusiastic tone to the tweet.\n",
      "\n",
      "3. Endorsement: The tweeter is positively endorsing the new spicy pickles from the company \"@PickleCo\", suggesting they enjoyed the product.\n",
      "\n",
      "4. Hashtags: The hashtags \"#pickleslove\" and \"#spicyfood\" reinforce the positive sentiment around the pickles and the tweeter's enjoyment of spicy food.\n",
      "\n",
      "Overall, the tweet expresses a very favorable sentiment towards the new spicy pickles from @PickleCo. The tweeter seems genuinely excited and pleased with the product, indicating a positive experience.\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze the sentiment in this tweet: Just tried the new spicy pickles from @PickleCo, and my taste buds are doing a happy dance! ðŸŒ¶ï¸ðŸ¥’ #pickleslove #spicyfood\"},\n",
    "    ]\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time I ran the above code, Claude generated this long response: \n",
    "```\n",
    "The sentiment in this tweet is overwhelmingly positive. The user expresses their enjoyment of the new spicy pickles from @PickleCo, using enthusiastic language and emojis to convey their delight.\n",
    "\n",
    "Positive indicators:\n",
    "1. \"My taste buds are doing a happy dance!\" - This phrase indicates that the user is extremely pleased with the taste of the pickles, to the point of eliciting a joyful physical response.\n",
    "\n",
    "2. Emojis - The use of the hot pepper ðŸŒ¶ï¸ and cucumber ðŸ¥’ emojis further emphasizes the user's excitement about the spicy pickles.\n",
    "\n",
    "3. Hashtags - The inclusion of #pickleslove and #spicyfood hashtags suggests that the user has a strong affinity for pickles and spicy food, and the new product aligns perfectly with their preferences.\n",
    "\n",
    "4. Exclamation mark - The exclamation mark at the end of the first sentence adds emphasis to the user's positive experience.\n",
    "\n",
    "Overall, the tweet conveys a strong sense of satisfaction, excitement, and enjoyment related to trying the new spicy pickles from @PickleCo.\n",
    "```\n",
    "\n",
    "This is a great response, but it's probably way more information than we need from Claude, especially if we're trying to automate the sentiment analysis of a large number of tweets.  \n",
    "\n",
    "We might prefer that Claude respond with a standardized output format like a single word (POSITIVE, NEUTRAL, NEGATIVE) or a numeric value (1, 0, -1).  For readability and simplicity, let's get Claude to respond with either \"POSITIVE\" or \"NEGATIVE\".  One way of doing this is through few-shot prompting.  We can provide Claude with a conversation history that shows exactly how we want it to respond: \n",
    "\n",
    "```py\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Unpopular opinion: Pickles are disgusting. Don't @ me\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"NEGATIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"I think my love for pickles might be getting out of hand. I just bought a pickle-shaped pool float\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"POSITIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"Seriously why would anyone ever eat a pickle?  Those things are nasty!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"NEGATIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"Just tried the new spicy pickles from @PickleCo, and my taste buds are doing a happy dance! ðŸŒ¶ï¸ðŸ¥’ #pickleslove #spicyfood\"},\n",
    "    ]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Unpopular opinion: Pickles are disgusting. Don't @ me\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"NEGATIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"I think my love for pickles might be getting out of hand. I just bought a pickle-shaped pool float\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"POSITIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"Seriously why would anyone ever eat a pickle?  Those things are nasty!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"NEGATIVE\"},\n",
    "        {\"role\": \"user\", \"content\": \"Just tried the new spicy pickles from @PickleCo, and my taste buds are doing a happy dance! ðŸŒ¶ï¸ðŸ¥’ #pickleslove #spicyfood\"},\n",
    "    ]\n",
    ")\n",
    "print(response.content[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
