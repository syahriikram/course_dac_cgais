





### Python Code:

import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Fetch API keys
openai_api_key = os.getenv("OPENAI_API_KEY")
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")

# Check if API keys are loaded
if openai_api_key and anthropic_api_key:
    print("‚úÖ API keys are successfully loaded.")
else:
    print("‚ö†Ô∏è Warning: One or more API keys are missing.")

# Optionally, display API keys (for debugging purposes only)
display_keys = False  # Change to True if you want to see the keys

if display_keys:
    print(f"OpenAI API Key: {openai_api_key}")
    print(f"Anthropic API Key: {anthropic_api_key}")
else:
    print("üîí API keys are loaded but hidden for security.")






from anthropic import Anthropic
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize the Anthropic client using environment variable
client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

# Call the Claude API
response = client.messages.create(
    model="claude-3-haiku-20240307",
    max_tokens=1000,
    messages=[
        {"role": "user", "content": "What flavors are used in Dr. Pepper?"}
    ]
)
print(response)


import os
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Set up OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Check if API key is loaded
if not client.api_key:
    raise ValueError("‚ö†Ô∏è OpenAI API key is missing. Please check your .env file.")

# Call the OpenAI API
response = client.chat.completions.create(
    model="gpt-4-turbo",  # You can also use "gpt-3.5-turbo"
    messages=[
        {"role": "user", "content": "What flavors are used in Dr. Pepper?"}
    ],
    max_tokens=1000
)

# Print response
print(response.choices[0].message.content)






from anthropic import Anthropic
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize the Anthropic client (not OpenAI)
client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

# Claude API with max_tokens and you can try max_tokens from 10 to 1000 and 500 is usually where the poem ends

truncated_response = client.messages.create(
    model="claude-3-haiku-20240307",
    max_tokens=1000,
    messages=[
        {"role": "user", "content": "Write me a poem"}
    ]
)
print(truncated_response.content[0].text)


import os
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize OpenAI client with API key from environment
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Check if API key is loaded correctly
if not client.api_key:
    raise ValueError("‚ö†Ô∏è OpenAI API key is missing. Please check your .env file.")

# Make API call with max_tokens set to 10 and try 100 or 500
truncated_response = client.chat.completions.create(
    model="gpt-4-turbo",
    max_tokens=1000,
    messages=[
        {"role": "user", "content": "Write me a poem"}
    ]
)

# Print truncated response
print(truncated_response.choices[0].message.content)






#Example of stop_sequence with Claude AI

from anthropic import Anthropic
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize the Anthropic client using environment variable
client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

response = client.messages.create(
    model="claude-3-haiku-20240307",
    max_tokens=500,
    messages=[{"role": "user", "content": "Generate a JSON object representing a person with a name, email, and phone number ."}],
)
print(response.content[0].text)


# Example of stop_sequence with OpenAI API

import os
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize the OpenAI client using environment variable
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# Make an API call with a stop sequence
response = client.chat.completions.create(
    model="gpt-4-turbo",  # Use "gpt-3.5-turbo" if needed
    max_tokens=500,
    messages=[
        {"role": "user", "content": "Generate a JSON object representing a person with a name, email, and phone number."}
    ],
    stop=["}"]  # Stop generation after the closing JSON brace
)

# Print response
print(response.choices[0].message.content)






#Example of Temperature with Claude AI

from anthropic import Anthropic
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize the Anthropic client using environment variable
client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

def demonstrate_temperature():
    temperatures = [0, 0.3, 0.5, 0.75, 1]
    for temperature in temperatures:
        print(f"Prompting Claude three times with temperature of {temperature}")
        print("================")
        for i in range(3):
            response = client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=100,
                messages=[{"role": "user", "content": "Come up with a name for an alien planet. Respond with a single word."}],
                temperature=temperature
            )
            print(f"Response {i+1}: {response.content[0].text}")

demonstrate_temperature()

#Notice that with a temperature of 0, all three responses are the same.  
#Note that even with a temperature of 0.0, the results will not be fully deterministic.  
#However, there is a clear difference when compared to the results with a temperature of 1.  
#Each response was a completely different alien planet name. 



# Example of Temperature with OpenAI API

import os
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize the OpenAI client using environment variable
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

def demonstrate_temperature():
    temperatures = [0, 0.3, 0.5, 0.75, 1]
    
    for temperature in temperatures:
        print(f"Prompting OpenAI GPT three times with temperature of {temperature}")
        print("================")
        
        for i in range(3):
            response = client.chat.completions.create(
                model="gpt-4-turbo",  # Use "gpt-3.5-turbo" if needed
                max_tokens=100,
                messages=[{"role": "user", "content": "Come up with a name for an golden retriever. Respond with a single word."}],
                temperature=temperature
            )
            
            print(f"Response {i+1}: {response.choices[0].message.content}")

# Run the function
demonstrate_temperature()






#Example of System Prompts with Claude AI

from anthropic import Anthropic
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize the Anthropic client using environment variable
client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

message = client.messages.create(
    model="claude-3-haiku-20240307",
    max_tokens=1000,
    system="You are a helpful foreign language tutor that always responds in Chinese.",
    messages=[
        {"role": "user", "content": "Hey there, how are you?!"}
    ]
)

print(message.content[0].text)


# Example of System Prompts with OpenAI API

import os
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize the OpenAI client using environment variable
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

# Create a chat completion with a system prompt
message = client.chat.completions.create(
    model="gpt-4-turbo",  # Use "gpt-3.5-turbo" if needed
    max_tokens=1000,
    messages=[
        {"role": "system", "content": "You are a helpful foreign language tutor that always responds in Chinese."},
        {"role": "user", "content": "Hey there, how are you?!"}
    ]
)

# Print response
print(message.choices[0].message.content)

